{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c62527",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308a87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbc3a5",
   "metadata": {},
   "source": [
    "#### Importing item.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef3e256",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './amazon-items.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./amazon-items.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m item\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './amazon-items.csv'"
     ]
    }
   ],
   "source": [
    "item = pd.read_csv('./amazon-items.csv')\n",
    "item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd57c8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47e914",
   "metadata": {},
   "source": [
    "#### Importing reviews.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c7154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review = pd.read_csv('./amazon-reviews.csv')\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b48d3c",
   "metadata": {},
   "source": [
    "#### Merging the two dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(review, item, how=\"left\", left_on=\"asin\", right_on=\"asin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda36e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"rating_x\": \"rating\", \"title_x\": \"title\", \"title_y\": \"item_title\", \"rating_y\": \"overall_rating\"}, inplace=True)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains {0[0]: .0f} rows and {0[1]: .0f} variables.\".format(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3be38",
   "metadata": {},
   "source": [
    "#### Checking the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf15016",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ef7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7a08d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082109c",
   "metadata": {},
   "source": [
    "#### Removing all unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aedcdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data = data[['asin', 'brand', 'rating', 'date', 'totalReviews', 'overall_rating', 'item_title', 'body']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cdfb2",
   "metadata": {},
   "source": [
    "#### Total Mobile count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "sns.countplot(x = 'brand', data =data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating per brand\n",
    "ax = data.groupby(\"brand\").mean()[\"rating\"].sort_values().plot(kind=\"barh\",\n",
    "                                                                figsize=(8,5), \n",
    "                                                                title=\"Average rating per Brand\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa58507",
   "metadata": {},
   "source": [
    "#### Extracting year and month from date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e42a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame which has date and year\n",
    "new = data[\"date\"].str.split(\",\", n = 1, expand = True) \n",
    "  \n",
    "# making separate date column from new data frame \n",
    "data[\"Dated\"]= new[0] \n",
    "  \n",
    "# making separate year column from new data frame \n",
    "data[\"year\"]= new[1] \n",
    "\n",
    "data=data.drop(['date'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the date \n",
    "new1 = data[\"Dated\"].str.split(\" \", n = 1, expand = True) \n",
    "  \n",
    "# adding month to the main dataset \n",
    "data[\"month\"]= new1[0] \n",
    "  \n",
    "# adding day to the main dataset \n",
    "data[\"day\"]= new1[1] \n",
    "\n",
    "data=data.drop(['Dated'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6d287",
   "metadata": {},
   "source": [
    "#### Ploting reviews over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93ccff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ax = pd.pivot_table(data, \n",
    "                    index=\"year\", \n",
    "                    columns=\"brand\", \n",
    "                    values=\"asin\", \n",
    "                    aggfunc=\"count\", \n",
    "                    fill_value=0).plot.area(title=\"Yearly Number of Reviews per Brand\", figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcd724",
   "metadata": {},
   "source": [
    "From this plot we can concluded that Samsung is the most rated brands, while Xiaomi has the highest average rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c96d9c",
   "metadata": {},
   "source": [
    "### Data  Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaning(text):\n",
    "   \n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body']=data['body'].apply(lambda x:review_cleaning(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43166839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer=ToktokTokenizer()\n",
    "\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094948e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "#Apply function on review column\n",
    "data['body'] = data['body'].apply(remove_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36027fe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#Apply function on review column\n",
    "data['body'] = data['body'].apply(simple_stemmer)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b4e10",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a82a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text= ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "data['body'] = data['body'].apply(lemmatize)\n",
    "print(data['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032aa4a",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05675d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    \n",
    "    if row['rating'] == 1.0 or row['rating'] == 2.0:\n",
    "        val = 'Negative'\n",
    "    elif row['rating'] == 3.0 or row['rating'] == 4.0 or row['rating'] == 5.0:\n",
    "        val = 'Positive'\n",
    "    else:\n",
    "        val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['sentiment'] = data.apply(f, axis=1)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3350b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelEncoder()\n",
    "data['sentiment'] = lb.fit_transform(data['sentiment'])\n",
    "print(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = data['body']\n",
    "    y = data['sentiment']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "    \n",
    "    print(X_train.shape,y_train.shape)\n",
    "    print(X_test.shape,y_test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053f390",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = data['body']\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=2)\n",
    "#words = model.wv.vocab\n",
    "\n",
    "vectors = model.wv['phone']\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de201687",
   "metadata": {},
   "source": [
    "### Bags of words model\n",
    "\n",
    "It is used to convert text documents to numerical vectors or bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29295324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(X_train)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(X_test)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719e04f",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency model (TFIDF)\n",
    "\n",
    "It is used to convert text documents to matrix of tfidf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(X_train)\n",
    "\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(X_test)\n",
    "\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e2139",
   "metadata": {},
   "source": [
    "### Machine Learning Classification\n",
    "\n",
    "#### Logistic regression model for both bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,y_train)\n",
    "print(lr_bow)\n",
    "\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,y_train)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e624e59",
   "metadata": {},
   "source": [
    "#### Logistic regression model performane on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff549bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfa074",
   "metadata": {},
   "source": [
    "#### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(y_test,lr_bow_predict)\n",
    "print(\"lr_bow_score :{:.2f}%\".format(lr_bow_score*100))\n",
    "\n",
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score : {:.2f}%\".format(lr_tfidf_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61dac7",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb7cf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(y_test,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "lr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67a2a5",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the linear svm\n",
    "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
    "\n",
    "#fitting the svm for bag of words\n",
    "svm_bow=svm.fit(cv_train_reviews,y_train)\n",
    "print(svm_bow)\n",
    "\n",
    "#fitting the svm for tfidf features\n",
    "svm_tfidf=svm.fit(tv_train_reviews,y_train)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870704e3",
   "metadata": {},
   "source": [
    "#### Model performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "svm_bow_predict=svm.predict(cv_test_reviews)\n",
    "print(svm_bow_predict)\n",
    "\n",
    "#Predicting the model for tfidf features\n",
    "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbc81e",
   "metadata": {},
   "source": [
    "#### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a359971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "svm_bow_score=accuracy_score(y_test,svm_bow_predict)\n",
    "print(\"svm_bow_score :{:.2f}%\".format(svm_bow_score*100))\n",
    "\n",
    "#Accuracy score for tfidf features\n",
    "svm_tfidf_score=accuracy_score(y_test,svm_tfidf_predict)\n",
    "print(\"svm_tfidf_score :{:.2f}%\".format(svm_tfidf_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311956a7",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "svm_bow_report=classification_report(y_test,svm_bow_predict,target_names=['Positive','Negative'])\n",
    "print(svm_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "svm_tfidf_report=classification_report(y_test,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(svm_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56335963",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes for bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e38cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,y_train)\n",
    "\n",
    "\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tv_train_reviews,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a012a8",
   "metadata": {},
   "source": [
    "#### Model performance on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e90e96",
   "metadata": {},
   "source": [
    "#### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(y_test,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :{:.2f}%\".format(mnb_bow_score*100))\n",
    "\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(y_test,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :{:.2f}%\".format(mnb_tfidf_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9eac0f",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(y_test,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "mnb_tfidf_report=classification_report(y_test,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f63f2a",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ef813",
   "metadata": {},
   "source": [
    "We can observed that multinomial naive bayes model performing well compared to logistic regression and linear support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5dcdc",
   "metadata": {},
   "source": [
    "## Sentimental Analysis using Vader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbb4eb",
   "metadata": {},
   "source": [
    "There are two common approaches for text sentiment analysis: the lexical method and the machine learning methhod.\n",
    "\n",
    "The lexcial method maps the new text to a pre-defined \"dictionary of sentiment\". VADER is one example of such method. Wtihe VADER, the sentiment score of a sentence is the normalised sum of sentiment scores of each word in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68654ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64351d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d89b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(df['body'].iloc[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.copy()\n",
    "dfs = dfs[['asin', 'brand', 'rating','overall_rating','title', 'item_title', 'body']]\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaning(text):\n",
    "\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['body']=dfs['body'].apply(lambda x:review_cleaning(x))\n",
    "dfs['title']=dfs['title'].apply(lambda x:review_cleaning(x))\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['title_score'] = dfs['title'].apply(lambda x: sid.polarity_scores(x))\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd64a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs['title_compound'] = dfs['title_score'].apply(lambda x: x['compound'])\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def f(dfs):\n",
    "        \n",
    "    if dfs['title_compound'] >= 0.05 :\n",
    "       value = \"Positive\"\n",
    " \n",
    "    elif dfs['title_compound'] <= - 0.05 :\n",
    "      value = \"Negative\"\n",
    " \n",
    "    else :\n",
    "       value = \"Neutral\"\n",
    "    \n",
    "    return value\n",
    "\n",
    "dfs['title_sentiment'] = dfs.apply(f, axis=1)\n",
    "dfs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23941ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['title_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52364ee3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs['body_score'] = dfs['body'].apply(lambda x: sid.polarity_scores(x))\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae34b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['body_compound'] = dfs['body_score'].apply(lambda x: x['compound'])\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cdc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def f(dfs):\n",
    "        \n",
    "    if dfs['body_compound'] >= 0.05 :\n",
    "       value = \"Positive\"\n",
    " \n",
    "    elif dfs['body_compound'] <= - 0.05 :\n",
    "      value = \"Negative\"\n",
    " \n",
    "    else :\n",
    "       value = \"Neutral\"\n",
    "    \n",
    "    return value\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0e8cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs['body_sentiment'] = dfs.apply(f, axis=1)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd8ec7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs['body_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa812cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "sns.countplot(x = 'brand', hue = 'body_sentiment', data = dfs)\n",
    "plt.xlabel('Comments Positive, Negative or Neutral', fontsize = 18)\n",
    "plt.ylabel('Count', fontsize = 18)\n",
    "plt.title('Comments sentiment analysis', fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "sns.countplot(x = 'brand', hue = 'title_sentiment', data = dfs)\n",
    "plt.xlabel('Title Positive, Negative or Neutral', fontsize = 18)\n",
    "plt.ylabel('Count', fontsize = 18)\n",
    "plt.title('Title sentiment analysis', fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e53c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(dfs['title_sentiment'], dfs['body_sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac238a",
   "metadata": {},
   "source": [
    "#### Conclusion for sentiment analysis using vader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b16bcf",
   "metadata": {},
   "source": [
    "As a result, we can conclude that the majority of the reviews and titles are positive, and the correlation between the rating and the sentiment of the reviews is also favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e378d",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"positivity\"] = reviews[\"rating\"].apply(lambda x: 1 if x>3 else(0 if x==3 else -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "punc = set(string.punctuation)\n",
    "keywords = reviews[\"brand\"].apply(lambda x: x.lower()).unique().tolist()\n",
    "keywords.append(\"phone\")\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    wordList = text.split()\n",
    "    \n",
    "    wordList = [\"\".join(x for x in word if (x==\"'\")|(x not in punc)) for word in wordList]\n",
    "   \n",
    "    wordList = [word for word in wordList if word not in stop]\n",
    "   \n",
    "    wordList = [word for word in wordList if word not in keywords]\n",
    "    \n",
    "    wordList = [lemma.lemmatize(word) for word in wordList]\n",
    "    return \" \".join(wordList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"body\"] = reviews[\"body\"].astype(\"str\")\n",
    "reviews[\"clean_text\"] = reviews[\"body\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9238c",
   "metadata": {},
   "source": [
    "##### Creating wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_dict(text):\n",
    "  \n",
    "    wordList = text.split()\n",
    "  \n",
    "    wordFreqDict = {word: wordList.count(word) for word in wordList}\n",
    "    \n",
    "    return wordFreqDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f54f16",
   "metadata": {},
   "source": [
    "##### Brand subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "apple = reviews[reviews[\"brand\"]==\"Apple\"].sort_values(by=[\"date\"], ascending=False)\n",
    "samsung = reviews[reviews[\"brand\"]==\"Samsung\"].sort_values(by=[\"date\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# Function to create a wordcloud from dictionary of word frequency\n",
    "def wordcloud_from_frequency(word_freq_dict, title, figure_size=(10, 6)):\n",
    "    wordcloud.generate_from_frequencies(word_freq_dict)\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot top10 positive words and top10 negative words in a grouped bar plot (from dictionaries)\n",
    "\n",
    "def topn_wordfreq_bar_both(pos_word_freq_dict, neg_word_freq_dict, pos_num_doc, neg_num_doc, topn, title, palette, height=6, aspect=2):\n",
    "    # Transforming positive word frequency into DF\n",
    "    df_pos = pd.DataFrame.from_dict(pos_word_freq_dict, orient=\"index\").sort_values(by=0, ascending=False).head(topn)\n",
    "    df_pos.columns = [\"frequency\"]\n",
    "    df_pos[\"frequency\"] = df_pos[\"frequency\"] / pos_num_doc\n",
    "    df_pos[\"label\"] = \"Positive\"\n",
    "    \n",
    "    # Transforming negative word frequency into DF\n",
    "    df_neg = pd.DataFrame.from_dict(neg_word_freq_dict, orient=\"index\").sort_values(by=0, ascending=False).head(topn)\n",
    "    df_neg.columns = [\"frequency\"]\n",
    "    df_neg[\"frequency\"] = df_neg[\"frequency\"] / neg_num_doc\n",
    "    df_neg[\"label\"] = \"Negative\"\n",
    "    \n",
    "    # Append two dataframes\n",
    "    df_append = df_pos.append(df_neg)\n",
    "    df_append.reset_index(inplace=True)\n",
    "    # Plot\n",
    "    sns.catplot(x=\"index\", y=\"frequency\", hue=\"label\", data=df_append, \n",
    "                kind=\"bar\",\n",
    "                palette=palette,\n",
    "                height=height, aspect=aspect, \n",
    "                legend_out=False)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for 1000 reviews for Apple\n",
    "\n",
    "apple_pos = \" \".join(apple[apple[\"positivity\"]==1][\"clean_text\"][0:1000])\n",
    "apple_pos_word_freq = word_freq_dict(apple_pos)\n",
    "wordcloud = WordCloud(width=5000, \n",
    "                      height=3000, \n",
    "                      max_words=200, \n",
    "                      colormap=\"Blues\",\n",
    "                      background_color=\"white\")\n",
    "wordcloud_from_frequency(apple_pos_word_freq, \"Most Frequent Words in the Latest 1000 Positive Reviews for Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple[apple[\"clean_text\"].apply(lambda x: \"new\" in x)][\"item_title\"].value_counts().sort_values(ascending=True).tail(10).plot(kind=\"barh\")\n",
    "plt.title(\"Most reviews that mention 'new' are from renewed iPhone buyers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple[\"renewed\"] = apple[\"item_title\"].apply(lambda x: (\"Renewed\" in x) | (\"Reburshied\" in x))\n",
    "print(\"{0: 0.1%} iPhones that were sold on Amazon are renewed/reburshied.\".format(apple[\"renewed\"].sum() / len(apple[\"renewed\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_neg = \" \".join(apple[apple[\"positivity\"]==-1][\"clean_text\"][0:1000])\n",
    "apple_neg_word_freq = word_freq_dict(apple_neg)\n",
    "wordcloud = WordCloud(width=5000, \n",
    "                      height=3000, \n",
    "                      max_words=200, \n",
    "                      colormap=\"Blues\",\n",
    "                      background_color=\"black\")\n",
    "wordcloud_from_frequency(apple_neg_word_freq, \"Most Frequent Words in the Latest 1000 Negative Reviews for Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn_wordfreq_bar_both(apple_pos_word_freq, apple_neg_word_freq, \n",
    "                       min(sum(apple[\"positivity\"]==1), 1000), \n",
    "                       min(sum(apple[\"positivity\"]==-1), 1000), \n",
    "                       10, \n",
    "                       \"Top10 Frequent Words in Latest Positive and Negative Reviews for Apple\", \n",
    "                       [\"lightblue\", \"lightcoral\"], \n",
    "                       height=6, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a42412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for 1000 reviews for Samsung\n",
    "\n",
    "samsung_pos = \" \".join(samsung[samsung[\"positivity\"]==1][\"clean_text\"][0:1000])\n",
    "samsung_pos_word_freq = word_freq_dict(samsung_pos)\n",
    "wordcloud = WordCloud(width=5000, \n",
    "                      height=3000, \n",
    "                      max_words=200, \n",
    "                      colormap=\"Greens\",\n",
    "                      background_color=\"white\")\n",
    "wordcloud_from_frequency(samsung_pos_word_freq, \"Most Frequent Words in the Latest 1000 Positive Reviews for Samsung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_neg = \" \".join(samsung[samsung[\"positivity\"]==-1][\"clean_text\"][0:1000])\n",
    "samsung_neg_word_freq = word_freq_dict(samsung_neg)\n",
    "wordcloud = WordCloud(width=5000, \n",
    "                      height=3000, \n",
    "                      max_words=200, \n",
    "                      colormap=\"Greens\",\n",
    "                      background_color=\"black\")\n",
    "wordcloud_from_frequency(samsung_neg_word_freq, \"Most Frequent Words in the Latest 1000 Negative Reviews for Samsung\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn_wordfreq_bar_both(samsung_pos_word_freq, samsung_neg_word_freq, \n",
    "                       min(sum(samsung[\"positivity\"]==1), 1000), \n",
    "                       min(sum(samsung[\"positivity\"]==-1), 1000), \n",
    "                       10, \n",
    "                       \"Top10 Frequent Words in Latest Positive and Negative Reviews for Samsung\", \n",
    "                       [\"steelblue\", \"orange\"], \n",
    "                       height=6, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec92c5f",
   "metadata": {},
   "source": [
    "The visulisations above show that:\n",
    "\n",
    "The most important considerations for cell phone buyers are battery health and screen condition.\n",
    "\n",
    "The majority of iPhones sold on Amazon are renewed/refurbished. Apple customers are satisfied if their purchases are in (near)    new condition, and they usually complain if there is a scratch on the screen or the battery health is poor.\n",
    "\n",
    "Samsung buyers are generally satisfied with the overall performance of their purchases, but they have complained about the        screen. Some customers also complained about unlocked phones sold by carriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6667f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
